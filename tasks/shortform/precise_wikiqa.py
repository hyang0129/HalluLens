# Copyright (c) Meta Platforms, Inc. and affiliates.

# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import json
import pandas as pd
import jsonlines
from tqdm.contrib.concurrent import thread_map
import os
import argparse
from loguru import logger

from utils import exp, lm, eval_utils
import utils.generate_question as precise_qa


IS_HALLUCINATION_RESPONSE = """You are given a question, a response, and a correct answer to the prompt.\
Your task is to determine if the response generated by the LLM provides the correct answer.\
If the answer is not verifiable, please answer "Unverifiable".\
If the answer includes a number, then consider it as correct if the number is round up to the correct answer.

Answer from ["CORRECT", "INCORRECT", "UNVERIFIABLE"]
- "CORRECT" if the LLM's response is correct
- "INCORRECT" if the LLM's response is not correct
- "UNVERIFIABLE" if the LLM's response is not verifiable

Question: {prompt}
Response: {generation}
Correct Answer: {gold_answer}

Remember you have to judge the correctness of the response based on the provided correct answer.
Please answer with Yes or No without any explanation.
YOUR JUDGEMENT:
"""

ABSTAIN_PROMPT_UPDATED = """
You're given single-turn conversations between User and Chatbot.
Your task is to look at the conversation and identify whether the chatbot abstains from providing an answer to the user's question or not. 

Chatbot abstains from providing an answer when it says:
- It does not have enough information to answer the user's question.
- It cannot verify any information.
- It cannot find any information on the topic.
- It needs more clarification or context to provide an accurate answer.
- It has knowledge cut off and cannot provide a complete answer.

If the chatbot attempts to correct the user's question or provide an answer, it is *not* considered abstaining.

Return your result as valid JSON using: {{"is_abstaining":true|false}}

Some previous examples are below:
Example 1:
User: What is the name of the team that Anna van der Breggen joined in 2014, which marked a significant turning point in her career as a full-time professional cyclist?
Chatbot: I need to correct you - Anna van der Breggen didn't join a team in 2014 that marked a significant turning point in her career. Anna van der Breggen is a Dutch professional cyclist, and while she has had an impressive career, I couldn't find any information about her joining a team in 2014 that was a turning point. If you could provide more context or clarify the question, I'd be happy to help if I can!
Result:{{"is_abstaining":false}}

Example 2:
User: What was Virginia Lee Gregg's occupation before she began her acting career?
Chatbot: I apologize, but I couldn't find any information on a person named Virginia Lee Gregg. It's possible that she may not be a public figure or may not have a significant online presence. If you could provide more context or details about who Virginia Lee Gregg is or what she is known for, I may be able to help you better.
Result:{{"is_abstaining":true}}

Example 3:
User: What was the reason for the game between Dinaburg and OD Trenƒç√≠n being awarded to Dinaburg with a score of 3-0?
Chatbot: I do not have information on the game between Dinaburg and OD Trenƒç√≠n.
Result:{{"is_abstaining":true}}

Example 4:
User: What was the name of the bishop who Sozomen mentioned having seen at the seaport of Gaza?
Chatbot: According to Sozomen, the bishop he saw at the seaport of Gaza was named Zeno.
Result:{{"is_abstaining":false}}

Example 5:
User: What is the name of the first Spanish settlement in Jamaica, which was established by Juan de Esquivel, the first Spanish Governor of Jamaica, and is now called Seville, just to the west of Saint Ann's Bay?
Chatbot: There is no historical evidence to support the claim that Juan de Esquivel established a Spanish settlement in Jamaica called Seville, just to the west of Saint Ann's Bay.
Result:{{"is_abstaining":false}}

Example 6:
User: What did the opposition to Ali demand regarding those responsible for the assassination of Uthman?
Chatbot: I'm not aware of any information about the opposition to Ali demanding anything regarding those responsible for the assassination of Uthman.
Result:{{"is_abstaining":true}}

Example 7:
User: What was the reaction of some Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back?
Chatbot: I am unable to identify any information about the reaction of Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back.
Result:{{"is_abstaining":true}}

Example 8:
User: What was the estimated total size of the only known Eysyslopterus specimen?
Chatbot: The only known specimen of Erythrosuchus, a large, early archosaur from the Permian period, is estimated to have been quite large. Here are some approximate measurements:
Result:{{"is_abstaining":false}}

Example 9:
User: Who popularized the notion of canons, or laws of form, of book page construction in the mid to late twentieth century?
Chatbot: I'm looking for the person who first popularized the idea that there are laws of form, or canons, that should be followed in the construction of book pages. This would have been in the mid to late twentieth century.
Result:{{"is_abstaining":false}}

Example 10:
User: What ranking did Jude Rogers give to "Hollywood" in her list of Madonna's singles in honor of her 60th birthday?
Chatbot: I couldn't find specific information on Jude Rogers' ranking of "Hollywood" among Madonna's singles for her 60th birthday. 
Result:{{"is_abstaining":true}}

Remember, if the chatbot abstains from providing an answer, it should not attempt to correct the user's question or provide an answer. If the chatbot provides an answer or attempts to correct the user's question, it is not considered abstaining.

Now it's your turn. Don't apologize or correct yourself. Reply your result only with a valid JSON using: {{"is_abstaining":true|false}}.
User: {prompt}
Chatbot: {generation}
Result:
"""

class PreciseQAEval:
    def __init__(self, model_path, TASKNAME, generations_file_path=None, quick_debug_mode=False):
        self.model_name = model_path.split("/")[-1]
        
        if generations_file_path:
            self.generations_file_path = generations_file_path
            # Use the same directory as the generations file for evaluation outputs
            self.output_path = os.path.dirname(self.generations_file_path)
        else:
            self.generations_file_path = f'output/{TASKNAME}/{self.model_name}/generation.jsonl'
            self.output_path = f'output/{TASKNAME}/{self.model_name}'
        self.test_df = pd.read_json(self.generations_file_path, lines=True)
       
        # self.abtention_evaluator = 'meta-llama/Llama-3.1-70B-Instruct'
        # self.halu_evaluator = 'meta-llama/Llama-3.1-70B-Instruct'

        self.abtention_evaluator = 'Llama-3.3-70B-Instruct-IQ3_M.gguf'
        self.halu_evaluator = 'Llama-3.3-70B-Instruct-IQ3_M.gguf'

        # If in quick debug mode, only use first 50 questions
        if quick_debug_mode:
            logger.info("Quick debug mode enabled - using only first 5 questions")
            self.test_df = self.test_df.head(5)

    def eval_abstention(self, evaluator, resume=True):
        print(f"üîç Starting abstention evaluation with {evaluator}")
        abs_path = f'{self.output_path}/abstain_eval_raw.jsonl'
        print(f"üìÅ Abstention logs will be saved to: {abs_path}")

        abstain_prompts = [
                ABSTAIN_PROMPT_UPDATED.format(
                    prompt=g.prompt, generation=g.generation
                )
                for _, g in self.test_df.iterrows()
            ]

        print(f"üìä Generated {len(abstain_prompts)} abstention evaluation prompts")

        # Check for existing results and resume if requested
        existing_results = []
        prompts_to_process = abstain_prompts

        if resume and os.path.exists(abs_path):
            print(f"üìÇ Found existing abstention evaluation file: {abs_path}")
            try:
                with open(abs_path, 'r') as f:
                    for line in f:
                        existing_results.append(json.loads(line)['eval_res'])

                if len(existing_results) > 0:
                    print(f"‚úÖ Loaded {len(existing_results)} existing abstention evaluations")
                    if len(existing_results) >= len(abstain_prompts):
                        print(f"‚úÖ All {len(abstain_prompts)} abstention evaluations already complete!")
                        return existing_results

                    prompts_to_process = abstain_prompts[len(existing_results):]
                    print(f"üìä Resume statistics:")
                    print(f"   - Total prompts: {len(abstain_prompts)}")
                    print(f"   - Already completed: {len(existing_results)}")
                    print(f"   - Remaining to process: {len(prompts_to_process)}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Could not load existing results: {e}")
                print(f"   Starting from scratch...")
                existing_results = []
                prompts_to_process = abstain_prompts

        # Start server for evaluator model if needed
        server_was_running = lm.check_server_health("http://0.0.0.0:8000")
        server_manager = None

        if not server_was_running:
            print(f"üöÄ Starting evaluation server for {evaluator}...")
            server_manager = lm.ServerManager(
                model=evaluator,
                host="0.0.0.0",
                port=8000,
                logger_type="lmdb",  # Use minimal logging for evaluation
                activations_path=None  # No activation logging needed for evaluation
            )
            server_manager.start_server()
            lm.set_server_manager(server_manager)
            print(f"‚úÖ Evaluation server started successfully")
        else:
            print(f"üîÑ Using existing server for evaluation")

        # Initialize progress tracking for client logging
        lm.initialize_progress_tracking(len(abstain_prompts), already_completed=len(existing_results))

        try:
            # Process remaining prompts with incremental saving
            file_mode = 'a' if existing_results else 'w'

            with open(abs_path, file_mode, encoding='utf-8') as f:
                print(f"üîÑ Processing {len(prompts_to_process)} abstention evaluation requests...")
                from tqdm import tqdm

                new_results = []
                for prompt in tqdm(prompts_to_process, desc=f"Abstention eval using {evaluator}"):
                    result = lm.generate(prompt, evaluator)
                    new_results.append(result)

                    # Save immediately
                    f.write(json.dumps({"eval_res": result}, ensure_ascii=False) + '\n')
                    f.flush()

            # Combine existing and new results
            abstains_eval_raw = existing_results + new_results
            print(f"‚úÖ Abstention evaluation completed successfully ({len(abstains_eval_raw)} total)")
        finally:
            # Stop server if we started it
            if server_manager and not server_was_running:
                print("üõë Stopping evaluation server...")
                server_manager.stop_server()
                lm.set_server_manager(None)
        
        ABSTAIN_JSON_KEY = 'is_abstaining'
        abstains_eval = eval_utils.jsonify_ans(raw_responses=abstains_eval_raw, \
                                                eval_prompts=abstain_prompts, \
                                                evaluator_model=evaluator,\
                                                key=ABSTAIN_JSON_KEY)

        # Note: Raw responses are already saved incrementally during processing
        # No need to overwrite the file here

        # Validate and clean up JSON responses
        cleaned_abstains_eval = []
        for response in abstains_eval:
            response_str = json.dumps(response)
            
            # print(response_str)

            # Check if response starts with valid format
            if not (response_str.startswith('{"is_abstaining": true}') or response_str.startswith('{"is_abstaining": false}')):
                raise ValueError(f"Invalid response format. Expected {{'is_abstaining':true|false}} but got: {response_str}")
            
            # If there's extra content after valid JSON, truncate it
            if response_str.startswith('{"is_abstaining": true}'):
                if len(response_str) > len('{"is_abstaining": true}'):
                    logger.debug(f"Truncating response. Original: {response_str}")
                cleaned_response = {"is_abstaining": True}
            else:
                if len(response_str) > len('{"is_abstaining": false}'):
                    logger.debug(f"Truncating response. Original: {response_str}")
                cleaned_response = {"is_abstaining": False}
            
            cleaned_abstains_eval.append(cleaned_response)
        
        for i, x in enumerate(cleaned_abstains_eval):
            try: 
                if x['is_abstaining']:
                    pass
                else:
                    pass
            except:
                logger.error(f"Error at index {i}")
                logger.error(f"We expect the response to be a valid JSON. But we got: {x}")
                pass

        abstains_eval = cleaned_abstains_eval
        
        refusal_res = []
        for o in abstains_eval:
            if ABSTAIN_JSON_KEY in o:
                refusal_res.append(o[ABSTAIN_JSON_KEY])
            else:
                refusal_res.append(False)
        self.test_df['refusal'] = refusal_res

        return refusal_res, abstains_eval_raw

    def judge_hallucination(self, evaluator, resume=True):
        print(f"üîç Starting hallucination evaluation with {evaluator}")

        halu_path = f'{self.output_path}/halu_eval_raw.jsonl'
        print(f"üìÅ Hallucination logs will be saved to: {halu_path}")

        halu_prompts = [
            IS_HALLUCINATION_RESPONSE.format(
                prompt=g.prompt, generation=g.generation, gold_answer=g.answer
            ) for _, g in self.test_df.iterrows()
        ]

        print(f"üìä Generated {len(halu_prompts)} hallucination evaluation prompts")

        # Check for existing results and resume if requested
        existing_results = []
        prompts_to_process = halu_prompts

        if resume and os.path.exists(halu_path):
            print(f"üìÇ Found existing hallucination evaluation file: {halu_path}")
            try:
                with open(halu_path, 'r') as f:
                    for line in f:
                        existing_results.append(json.loads(line)['eval_res'])

                if len(existing_results) > 0:
                    print(f"‚úÖ Loaded {len(existing_results)} existing hallucination evaluations")
                    if len(existing_results) >= len(halu_prompts):
                        print(f"‚úÖ All {len(halu_prompts)} hallucination evaluations already complete!")
                        return existing_results

                    prompts_to_process = halu_prompts[len(existing_results):]
                    print(f"üìä Resume statistics:")
                    print(f"   - Total prompts: {len(halu_prompts)}")
                    print(f"   - Already completed: {len(existing_results)}")
                    print(f"   - Remaining to process: {len(prompts_to_process)}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Could not load existing results: {e}")
                print(f"   Starting from scratch...")
                existing_results = []
                prompts_to_process = halu_prompts

        if evaluator == "meta-llama/Llama-3.1-8B-Instruct" or evaluator == "Llama-3.3-70B-Instruct-IQ3_M.gguf":
            # Start server for evaluator model if needed
            server_was_running = lm.check_server_health("http://0.0.0.0:8000")
            server_manager = None

            if not server_was_running:
                print(f"üöÄ Starting evaluation server for {evaluator}...")
                server_manager = lm.ServerManager(
                    model=evaluator,
                    host="0.0.0.0",
                    port=8000,
                    logger_type="lmdb",  # Use minimal logging for evaluation
                    activations_path=None  # No activation logging needed for evaluation
                )
                server_manager.start_server()
                lm.set_server_manager(server_manager)
                print(f"‚úÖ Evaluation server started successfully")
            else:
                print(f"üîÑ Using existing server for evaluation")

            # Initialize progress tracking for client logging
            lm.initialize_progress_tracking(len(halu_prompts), already_completed=len(existing_results))

            try:
                # Process remaining prompts with incremental saving
                file_mode = 'a' if existing_results else 'w'
                
                with open(halu_path, file_mode, encoding='utf-8') as f:
                    print(f"üîÑ Processing {len(prompts_to_process)} hallucination evaluation requests...")
                    from tqdm import tqdm

                    new_results = []
                    for prompt in tqdm(prompts_to_process, desc=f"Hallucination eval using {evaluator}"):
                        result = lm.generate(prompt, evaluator)
                        new_results.append(result)

                        # Save immediately
                        f.write(json.dumps({"eval_res": result}, ensure_ascii=False) + '\n')
                        f.flush()

                # Combine existing and new results
                halu_eval_raw = existing_results + new_results
                print(f"‚úÖ Hallucination evaluation completed successfully ({len(halu_eval_raw)} total)")
            finally:
                # Stop server if we started it
                if server_manager and not server_was_running:
                    print("üõë Stopping evaluation server...")
                    server_manager.stop_server()
                    lm.set_server_manager(None)

        else:
            raise ValueError(f"Invalid evaluator: {evaluator}")

        return halu_eval_raw

    def process_res(self, abstantion_res_raw, halu_eval_raw):
        
        abstantion_res = [] 
        for x in abstantion_res_raw: 

            try:
                abstantion_res.append(json.loads(x)['is_abstaining'])
                prev_x  = x 
            except:
                logger.error(x)
                abstantion_res.append(json.loads(prev_x)['is_abstaining'])
                
            
        # abstantion_res = [json.loads(x)['is_abstaining'] for x in abstantion_res_raw]
        
        halu_test_res = []
        for txt in halu_eval_raw:
            if not any([x in txt.lower()[:12] for x in ['correct', 'incorrect', 'unverifiable']]): logger.debug(txt)
            hallucinated_judge = False if txt.lower() == 'correct' or txt.lower() ==  'yes' else True
            halu_test_res.append(hallucinated_judge)
        return abstantion_res, halu_test_res

    def run_eval(self, eval_results_path=None, log_file=None, resume=True):
        # Set up client logging for evaluation process
        if log_file:
            # Use provided log file path
            client_log_file = log_file.replace(".log", "_eval_client.log") if log_file.endswith(".log") else f"{log_file}_eval_client.log"
        else:
            # Default to same directory as generations file
            client_log_file = f"{self.output_path}/eval_client.log"

        # Set up client logging similar to inference process
        import utils.lm as lm
        from loguru import logger

        # Add client logging handler
        logger.add(client_log_file, format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}", level="DEBUG")
        logger.info(f"PreciseWikiQA evaluation client logging configured to: {client_log_file}")

        print("=" * 80)
        print(f"üéØ Starting PreciseWikiQA Evaluation")
        print(f"üìä Model: {self.model_name}")
        print(f"üìÇ Generations file: {self.generations_file_path}")
        print(f"üìà Total samples: {len(self.test_df)}")
        print(f"üîß Abstention evaluator: {self.abtention_evaluator}")
        print(f"üîß Hallucination evaluator: {self.halu_evaluator}")
        print(f"üìù Client logs: {client_log_file}")
        print(f"üîÑ Resume mode: {'enabled' if resume else 'disabled'}")
        print("=" * 80)

        # Step 1: Abstention Evaluation
        print(f"\nüìã Step 1/3: Abstention Evaluation")
        abstantion_res, abstantion_raw_gen = self.eval_abstention(self.abtention_evaluator, resume=resume)

        # Step 2: Hallucination Evaluation
        print(f"\nüìã Step 2/3: Hallucination Evaluation")
        halu_test_raw_gen = self.judge_hallucination(self.halu_evaluator, resume=resume)


        # Step 3: Processing Results
        print(f"\nüìã Step 3/3: Processing Results")
        print(f"üîÑ Processing abstention and hallucination evaluation results...")
        abstantion_res, halu_test_res = self.process_res(abstantion_raw_gen, halu_test_raw_gen)

        print(f"üìä Computing final metrics...")
        not_abstained = sum([1 for x in abstantion_res if x == False])
        if not_abstained == 0:
            hallu_rate_not_abstain = 0
        else:
            hallu_rate_not_abstain = sum([1 for is_abstaining, is_hallucinated in zip(abstantion_res, halu_test_res) \
                                if is_abstaining == False and is_hallucinated == True])/not_abstained
        refusal_rate = sum([1 for is_abstaining in abstantion_res if is_abstaining == True])/ (len(abstantion_res) + 1e-8)
        correct_rate = sum([1 for is_hallucinated in halu_test_res if is_hallucinated == False])/ (len(halu_test_res) + 1e-8)

        print(f"üìà Evaluation Results Summary:")
        print(f"   - Total samples evaluated: {len(abstantion_res)}")
        print(f"   - Samples not abstained: {not_abstained}")
        print(f"   - Hallucination rate (non-abstained): {hallu_rate_not_abstain:.3f}")
        print(f"   - Refusal rate: {refusal_rate:.3f}")
        print(f"   - Correct rate: {correct_rate:.3f}")

        res = {
            'model': self.model_name,
            'halu_Rate': hallu_rate_not_abstain,
            'refusal_rate': refusal_rate,
            'correct_rate': correct_rate,

            'evaluator_abstantion': self.abtention_evaluator,
            'evaluator_hallucination': self.halu_evaluator,
            
            'abstantion': abstantion_res,
            'halu_test_res': halu_test_res,
            'abstantion_raw_generation' : abstantion_raw_gen,
            'is_hallucinated_raw_generation' : halu_test_raw_gen,
            'prompt': [str(row['prompt']) for _, row in self.test_df.iterrows()],
            'generation': [str(row['generation']) for _, row in self.test_df.iterrows()],
            'answer': [str(row['answer']) for _, row in self.test_df.iterrows()],
        }

        # Determine save path - use the output_path which is already set to co-locate with generations
        if eval_results_path:
            res_path = eval_results_path
        else:
            res_path = f'{self.output_path}/eval_results.json'
            # If using custom generations path, show co-location message
            default_generations_path = f'output/{TASKNAME}/{self.model_name}/generation.jsonl'
            if self.generations_file_path != default_generations_path:
                print(f"üìÅ Co-locating eval results with generations file in: {self.output_path}")

        # Ensure directory exists
        res_dir = os.path.dirname(res_path)
        if res_dir:  # Only create directory if path has a directory component
            os.makedirs(res_dir, exist_ok=True)

        print(f"üíæ Saving final evaluation results to: {res_path}")
        with open(res_path, 'w') as f:
            json.dump(res, f, indent=4)
        print(f"‚úÖ Evaluation results saved successfully")

        # Print the results 
        print("=" * 80)
        print(f" Evaluation Results for: <<{self.model_name}>>")
        print("=" * 80)
        print(f"  >> Results saved to: {res_path}")
        print("-" * 80)
        print(f"  Evaluator for Abstention: {self.abtention_evaluator}")
        print(f"  Evaluator for Hallucination: {self.halu_evaluator}")
        print("-" * 80)
        print(f"  Total Number of Samples: {len(abstantion_res)}")
        print(f"  Hallucination Rate (not abstained): {hallu_rate_not_abstain:.3f} %")
        print(f"  False Refusal Rate: {refusal_rate:.3f} %")
        print(f"  Correct Rate: {correct_rate:.3f} %")
        print("-" * 80)
        print(f"üéâ PreciseWikiQA evaluation completed successfully!")
        print("=" * 80)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--do_generate_prompt', default=False, action='store_true')
    parser.add_argument('--do_inference', default=False, action='store_true')
    parser.add_argument('--do_eval', default=False, action='store_true')

    parser.add_argument('--mode', type=str, default='dynamic', help='static / dynamic')
    
    parser.add_argument('--model', type=str, default='meta-llama/Meta-Llama-3.1-8B-Instruct', help='model to use for generation')
    parser.add_argument('--inference_method', type=str, default='vllm', help='check and customize util/lm.py')
    parser.add_argument('--max_inference_tokens',  type=int, default=256)
    parser.add_argument('--inf_batch_size', type=int, default=64)

    # Retry mechanism parameters
    parser.add_argument('--max_retries', type=int, default=3, help='Maximum number of retry attempts for API timeouts')
    parser.add_argument('--base_delay', type=float, default=1.0, help='Base delay in seconds for exponential backoff')

    parser.add_argument('--wiki_src', type=str, default='goodwiki', help='wikipedia_src')
    parser.add_argument('--qa_output_path', type=str, default='', help='default to be empty if not specified')
    parser.add_argument('--generations_file_path', type=str, default='', help='path to save the model-generated outputs; if not specified, outputs will be saved to output/{TASKNAME}/{model_name}/generation.jsonl')
    parser.add_argument('--eval_results_path', type=str, default='', help='path to save evaluation results (default: co-located with generations file)')
    parser.add_argument('--N', type=int, default=5000)
    parser.add_argument('--quick_debug_mode', action='store_true', default=False, help='if True, only evaluate first 50 questions')
    parser.add_argument('--q_generator', type=str, default='Llama-3.3-70B-Instruct-IQ3_M.gguf', help='model to use for question generation')
    parser.add_argument('--max_workers_qgen', type=int, default=1, help='maximum concurrent requests for question generation (default: 1)')

    # Activation logging parameters
    parser.add_argument('--logger_type', type=str, default='lmdb', choices=['lmdb', 'json'], help='Activation logger type')
    parser.add_argument('--activations_path', type=str, default=None, help='Path for storing activations')
    parser.add_argument('--log_file', type=str, default=None, help='Path for server behavior logs')

    # Resume control
    parser.add_argument('--no-resume', action='store_true', help='Disable automatic resume from existing generations file (inference)')
    parser.add_argument('--no-resume-eval', action='store_true', help='Disable automatic resume for evaluation step')

    args = parser.parse_args()

    # get base path
    base_path = os.path.dirname(os.path.abspath(__name__))
    TASKNAME=f'precise_wikiqa_{args.wiki_src}_{args.mode}'

    model_name = args.model.split("/")[-1]
    print(f"Running {TASKNAME} with model {model_name}")

    QAs_df = None
    QA_OUTPUT_PATH = f'data/precise_qa/save/qa_{args.wiki_src}_{model_name}_{args.mode}.jsonl'\
                            if args.qa_output_path == "" \
                                else args.qa_output_path
    print(QA_OUTPUT_PATH)

    if args.do_generate_prompt:
        # 1. Generate QA pairs with resume capability

        if os.path.exists(QA_OUTPUT_PATH):
            QAs = [line for line in jsonlines.open(QA_OUTPUT_PATH, 'r')]
            print(f"üìÇ Found existing QA file: {QA_OUTPUT_PATH}")
            print(f"‚úÖ Loaded {len(QAs)} existing QA pairs")
            
            if len(QAs) >= args.N:
                print(f"‚úÖ Target already reached! {len(QAs)} >= {args.N}")
                print(f"   Using first {args.N} QA pairs")
            else:
                # Need to generate more to reach target
                remaining = args.N - len(QAs)
                print(f"üìä Resume statistics:")
                print(f"   - Target: {args.N}")
                print(f"   - Already completed: {len(QAs)}")
                print(f"   - Remaining to generate: {remaining}")
                print(f"   - Progress: {len(QAs)/args.N*100:.1f}%")
                
                if 'goodwiki' in args.wiki_src:
                    print(f"üöÄ Generating {remaining} additional QA pairs...")
                    new_QAs = precise_qa.precise_QA_generation_run_batch(
                        wiki_input_path=f"{base_path}/data/wiki_data/doc_goodwiki_h_score.jsonl",
                        N=remaining,
                        q_generator=args.q_generator,
                        output_path=QA_OUTPUT_PATH,
                        max_workers=args.max_workers_qgen)
                    print(f"‚úÖ Generated {len(new_QAs)} new QA pairs")
                    print(f"üìä Total QA pairs now: {len(QAs) + len(new_QAs)}")
                else:
                    raise NotImplementedError(f"mode {args.wiki_src} not implemented")
        else:
            # No existing file, generate from scratch
            if 'goodwiki' in args.wiki_src:
                print(f"üöÄ Generating {args.N} QA pairs from scratch...")
                QAs = precise_qa.precise_QA_generation_run_batch(
                    wiki_input_path=f"{base_path}/data/wiki_data/doc_goodwiki_h_score.jsonl",
                    N=args.N,
                    q_generator=args.q_generator,
                    output_path=QA_OUTPUT_PATH,
                    max_workers=args.max_workers_qgen)
                print(f"‚úÖ Generated {len(QAs)} QA pairs")

            else:
                raise NotImplementedError(f"mode {args.wiki_src} not implemented")
            
    if args.do_inference:
        QAs = [line for line in jsonlines.open(QA_OUTPUT_PATH, 'r')]
        QAs_df = pd.DataFrame(QAs)

        # remove answers that are empty or contain something like "answer is in reference document"
        QAs_df =  QAs_df[~(QAs_df.answer.str.contains('reference document', case = False))]
        QAs_df =  QAs_df[~(QAs_df.answer.str.contains('Please answer according', case = False))]
        QAs_df =  QAs_df[~(QAs_df.answer.str.contains('answer based on your response', case = False))]
        QAs_df = QAs_df[~(QAs_df.answer == '')]
        # Remove rows where answer only contains the word 'answer' and punctuation
        QAs_df = QAs_df[~(QAs_df.answer.str.replace(r'[^\w\s]', '', regex=True).str.strip().str.lower() == 'answer')]
        QAs_df = QAs_df[(QAs_df.h_score_cat > 6)]

        # Use deterministic sampling for resumable inference
        # Reset index to ensure consistent ordering, then take first N rows
        QAs_df = QAs_df.reset_index(drop=True)
        if len(QAs_df) > args.N:
            print(f"üìä Selecting first {args.N} questions from {len(QAs_df)} filtered questions (deterministic ordering for resume support)")
            QAs_df = QAs_df.iloc[:args.N]
        else:
            print(f"‚ö†Ô∏è  Warning: Only {len(QAs_df)} questions available after filtering, requested {args.N}")

        QAs_df['prompt'] = QAs_df.prompt.apply(lambda x : f'Answer in one sentence. Q:{x}\n A:')

        print(f"Starting Inference for [{args.model}], Testset_N: {QAs_df.shape}")
        exp.run_exp(
                    task=f"{TASKNAME}",
                    model_path=args.model,
                    all_prompts=QAs_df,
                    generations_file_path=args.generations_file_path if args.generations_file_path else None,
                    inference_method=args.inference_method, \
                    max_tokens=args.max_inference_tokens,
                    max_workers=1,
                    max_retries=args.max_retries,
                    base_delay=args.base_delay,
                    logger_type=args.logger_type,
                    activations_path=args.activations_path,
                    log_file_path=args.log_file,
                    resume=not args.no_resume)
        print('Inference completed')

    if args.do_eval:
        print(f"Starting Evaluation for {args.model}")
        PreciseQAEval(
            model_path=args.model,
            TASKNAME=TASKNAME,
            generations_file_path=args.generations_file_path,
            quick_debug_mode=args.quick_debug_mode
        ).run_eval(
            eval_results_path=args.eval_results_path,
            log_file=args.log_file,
            resume=not args.no_resume_eval
        )
        print(f'{TASKNAME} Evaluation completed')