{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ceb155e",
   "metadata": {},
   "source": [
    "# Layer-aware contrastive training (new Trainer + evaluation)\n",
    "\n",
    "This notebook mirrors `b_contrastive_training_with_new_trainer.ipynb`, but uses layer-aware components:\n",
    "- `activation_research.model.LayerAwareProgressiveCompressor`\n",
    "- `activation_research.trainer.LayerAwareContrastiveTrainer`\n",
    "\n",
    "It assumes activations + eval results are already on disk (for example a `.zarr` store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from activation_logging.activation_parser import ActivationParser\n",
    "from activation_research.model import LayerAwareProgressiveCompressor\n",
    "from activation_research.trainer import LayerAwareContrastiveTrainer, LayerAwareContrastiveTrainerConfig\n",
    "from activation_research.metric_evaluator import MultiMetricHallucinationEvaluator\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths (edit these for your environment) ----\n",
    "inference_json = 'shared/goodwiki_jsonv2/generation.jsonl'\n",
    "eval_json = 'shared/goodwiki.zarr/eval_results.json'\n",
    "activations_path = 'shared/goodwiki.zarr/activations.zarr'\n",
    "\n",
    "# ---- Dataset parameters ----\n",
    "backend = 'zarr'  # 'zarr' or 'wds' or 'auto'\n",
    "relevant_layers = list(range(14, 30))\n",
    "target_layers = [22, 26]  # used for embedding-based OOD evaluation\n",
    "\n",
    "# Treat one class as outlier for certain evaluations.\n",
    "# If outlier_class=1, we use non-halu samples as baseline (ID).\n",
    "outlier_class = 1\n",
    "\n",
    "# Optional: if set, one of the two views is always from this layer index.\n",
    "fixed_layer = None\n",
    "\n",
    "# ---- Layer-aware model hyperparams ----\n",
    "device = 'auto'  # 'auto', 'cuda', 'cpu'\n",
    "input_dim = 4096\n",
    "final_dim = 512\n",
    "layer_embed_dim = 128\n",
    "conditioning = 'film_both'  # film_in | film_out | film_both | positional | concatenate\n",
    "\n",
    "# If parser emits absolute layer ids, this should cover max index.\n",
    "# Works for both absolute and compact indices in practice.\n",
    "num_layers = max(relevant_layers) + 1\n",
    "\n",
    "# ---- Training hyperparams ----\n",
    "max_epochs = 50\n",
    "batch_size = 512\n",
    "lr = 1e-5\n",
    "temperature = 0.25\n",
    "steps_per_epoch_override = 200  # e.g., 1000 for fixed steps/epoch\n",
    "\n",
    "num_workers = 30\n",
    "persistent_workers = True\n",
    "\n",
    "checkpoint_dir = os.path.join('checkpoints', 'contrastive_layer_aware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cde6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load metadata + build train/test datasets ----\n",
    "ap = ActivationParser(\n",
    "    inference_json=inference_json,\n",
    "    eval_json=eval_json,\n",
    "    activations_path=activations_path,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "train_dataset = ap.get_dataset(\n",
    "    'train',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "test_dataset = ap.get_dataset(\n",
    "    'test',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "print('train:', len(train_dataset))\n",
    "print('test :', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Layer-aware contrastive encoder ----\n",
    "model = LayerAwareProgressiveCompressor(\n",
    "    num_layers=num_layers,\n",
    "    input_dim=input_dim,\n",
    "    final_dim=final_dim,\n",
    "    layer_embed_dim=layer_embed_dim,\n",
    "    conditioning=conditioning,\n",
    "    input_dropout=0.3,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c28495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train with the layer-aware Trainer API ----\n",
    "config = LayerAwareContrastiveTrainerConfig(\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    temperature=temperature,\n",
    "    steps_per_epoch_override=steps_per_epoch_override,\n",
    "    device=device,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    save_every=1,\n",
    "    snapshot_every=10,\n",
    "    snapshot_keep_last=5,\n",
    "\n",
    "    # Supervised contrastive: uses `halu` labels; ignore the configured outlier class.\n",
    "    use_labels=True,\n",
    "    ignore_label=outlier_class,\n",
    "\n",
    "    # Keeps DataLoader workers alive for map-style datasets (disabled automatically for IterableDataset).\n",
    "    use_infinite_index_stream=True,\n",
    "    use_infinite_index_stream_eval=True,\n",
    ")\n",
    "print(config)\n",
    "\n",
    "trainer = LayerAwareContrastiveTrainer(model, config=config)\n",
    "trainer.fit(train_dataset=train_dataset, val_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e350b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OOD evaluation with the new evaluator abstraction ----\n",
    "train_dataset_for_inference = ap.get_dataset(\n",
    "    'train',\n",
    "    relevant_layers=target_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "eval_dataset = ap.get_dataset(\n",
    "    'test',\n",
    "    relevant_layers=target_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "train_loader_for_baseline = DataLoader(train_dataset_for_inference, batch_size=64, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model_for_eval = trainer.model\n",
    "\n",
    "ood_eval = MultiMetricHallucinationEvaluator(\n",
    "    activation_parser_df=ap.df,\n",
    "    train_data_loader=train_loader_for_baseline,\n",
    "    layers=None,\n",
    "    batch_size=256,\n",
    "    sub_batch_size=64,\n",
    "    device=str(trainer.device),\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False,\n",
    "    outlier_class=outlier_class,\n",
    "    metrics=[\n",
    "        'cosine',\n",
    "        'mds',\n",
    "        {\n",
    "            'metric': 'knn',\n",
    "            'kwargs': {\n",
    "                'k': 50,\n",
    "                'metric': 'euclidean',\n",
    "                'calibrate_k': True,\n",
    "                'k_candidates': [50, 100, 200, 500, 1000],\n",
    "                'max_train_size': 200000,\n",
    "                'sample_seed': 0,\n",
    "            },\n",
    "            'train_selection': 'all',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "ood_stats = ood_eval.compute(eval_loader, model_for_eval)\n",
    "print('OOD metrics:', ood_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9525d5f",
   "metadata": {},
   "source": [
    "## Evaluate across epoch snapshots\n",
    "\n",
    "Load each saved snapshot checkpoint, run OOD evaluation, and compare metrics across training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Discover available snapshots ----\n",
    "snapshot_pattern = os.path.join(checkpoint_dir, 'layer_aware_contrastive_snapshot_epoch_*.pt')\n",
    "snapshot_files = sorted(glob.glob(snapshot_pattern))\n",
    "\n",
    "last_ckpt = os.path.join(checkpoint_dir, 'layer_aware_contrastive_last.pt')\n",
    "if os.path.exists(last_ckpt):\n",
    "    snapshot_files.append(last_ckpt)\n",
    "\n",
    "def _parse_epoch(path: str) -> str:\n",
    "    basename = os.path.basename(path)\n",
    "    m = re.search(r'epoch_(\\d+)', basename)\n",
    "    if m:\n",
    "        return f\"epoch_{int(m.group(1))}\"\n",
    "    if 'last' in basename:\n",
    "        return 'last'\n",
    "    return basename\n",
    "\n",
    "snapshot_info = [(p, _parse_epoch(p)) for p in snapshot_files]\n",
    "print(f'Found {len(snapshot_info)} checkpoint(s) to evaluate:')\n",
    "for path, label in snapshot_info:\n",
    "    print(f'  {label:>12s}  ->  {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac31c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluate each snapshot ----\n",
    "if 'train_dataset_for_inference' not in dir():\n",
    "    train_dataset_for_inference = ap.get_dataset(\n",
    "        'train', relevant_layers=target_layers, fixed_layer=fixed_layer, backend=backend,\n",
    "    )\n",
    "if 'eval_dataset' not in dir():\n",
    "    eval_dataset = ap.get_dataset(\n",
    "        'test', relevant_layers=target_layers, fixed_layer=fixed_layer, backend=backend,\n",
    "    )\n",
    "\n",
    "train_loader_for_baseline = DataLoader(train_dataset_for_inference, batch_size=64, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "eval_device = str(trainer.device) if 'trainer' in dir() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "map_location = eval_device\n",
    "\n",
    "for ckpt_path, epoch_label in tqdm(snapshot_info, desc='Evaluating snapshots'):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "    snapshot_model = LayerAwareProgressiveCompressor(\n",
    "        num_layers=num_layers,\n",
    "        input_dim=input_dim,\n",
    "        final_dim=final_dim,\n",
    "        layer_embed_dim=layer_embed_dim,\n",
    "        conditioning=conditioning,\n",
    "        input_dropout=0.3,\n",
    "    )\n",
    "    snapshot_model.load_state_dict(ckpt['model_state_dict'])\n",
    "    snapshot_model = snapshot_model.to(eval_device)\n",
    "    snapshot_model.eval()\n",
    "\n",
    "    ood_eval = MultiMetricHallucinationEvaluator(\n",
    "        activation_parser_df=ap.df,\n",
    "        train_data_loader=train_loader_for_baseline,\n",
    "        layers=None,\n",
    "        batch_size=256,\n",
    "        sub_batch_size=64,\n",
    "        device=eval_device,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=False,\n",
    "        outlier_class=outlier_class,\n",
    "        metrics=[\n",
    "            'cosine',\n",
    "            'mds',\n",
    "            {\n",
    "                'metric': 'knn',\n",
    "                'kwargs': {\n",
    "                    'k': 50,\n",
    "                    'metric': 'euclidean',\n",
    "                    'calibrate_k': True,\n",
    "                    'k_candidates': [50, 100, 200, 500, 1000],\n",
    "                    'max_train_size': 200000,\n",
    "                    'sample_seed': 0,\n",
    "                },\n",
    "                'train_selection': 'all',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    stats = ood_eval.compute(eval_loader, snapshot_model)\n",
    "\n",
    "    epoch_num = int(re.search(r'\\d+', epoch_label).group()) if re.search(r'\\d+', epoch_label) else 9999\n",
    "    result_row = {'checkpoint': epoch_label, 'epoch': epoch_num, 'path': ckpt_path}\n",
    "    result_row.update(stats)\n",
    "    all_results.append(result_row)\n",
    "\n",
    "    logger.info(f'[{epoch_label}] {stats}')\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values('epoch').reset_index(drop=True)\n",
    "print('\\n=== Results across snapshots ===')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c811c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize metrics across epochs ----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "meta_cols = {'checkpoint', 'epoch', 'path'}\n",
    "metric_cols = [c for c in results_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(results_df[c])]\n",
    "\n",
    "if not metric_cols:\n",
    "    print('No numeric metric columns found to plot.')\n",
    "else:\n",
    "    n_metrics = len(metric_cols)\n",
    "    fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 4), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, col in zip(axes, metric_cols):\n",
    "        ax.plot(results_df['epoch'], results_df[col], marker='o', linewidth=1.5)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(col)\n",
    "        ax.set_title(col)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('OOD Metrics vs Training Epoch (Layer-aware)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
