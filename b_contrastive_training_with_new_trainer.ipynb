{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9438da",
   "metadata": {},
   "source": [
    "# Contrastive training (new Trainer + evaluation)\n",
    "\n",
    "This notebook mirrors the flow of `a_preparing_training_precise_qa_halu_as_outlier(1).ipynb`, but uses:\n",
    "- `activation_research.trainer.ContrastiveTrainer` for contrastive training\n",
    "- `activation_research.metric_evaluator.MultiMetricHallucinationEvaluator` for multi-metric OOD evaluation (cosine / Mahalanobis / KNN)\n",
    "\n",
    "It assumes activations + eval results are already on disk (e.g. a `.zarr` store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbe1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from activation_logging.activation_parser import ActivationParser\n",
    "from activation_research.model import ProgressiveCompressor\n",
    "from activation_research.trainer import ContrastiveTrainer, ContrastiveTrainerConfig\n",
    "from activation_research.metric_evaluator import MultiMetricHallucinationEvaluator\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths (edit these for your environment) ----\n",
    "inference_json = 'shared/goodwiki_jsonv2/generation.jsonl'\n",
    "eval_json = 'shared/goodwiki.zarr/eval_results.json'\n",
    "activations_path = 'shared/goodwiki.zarr/activations.zarr'\n",
    "\n",
    "# ---- Dataset parameters ----\n",
    "backend = 'zarr'  # 'zarr' or 'wds' or 'auto'\n",
    "relevant_layers = list(range(14, 30))\n",
    "target_layers = [22, 26]  # used for embedding-based OOD evaluation\n",
    "\n",
    "# Treat one class as outlier for certain evaluations.\n",
    "# If outlier_class=1, we use non-halu samples as baseline (ID).\n",
    "outlier_class = 1\n",
    "\n",
    "# Optional: if set, one of the two views is always from this index-in-relevant_layers\n",
    "# (matches ActivationParser semantics).\n",
    "fixed_layer = None\n",
    "\n",
    "# ---- Model / training hyperparams ----\n",
    "device = 'auto'  # 'auto', 'cuda', 'cpu'\n",
    "input_dim = 4096\n",
    "final_dim = 512\n",
    "\n",
    "max_epochs = 50\n",
    "batch_size = 512\n",
    "lr = 1e-5\n",
    "temperature = 0.25\n",
    "steps_per_epoch_override = None  # e.g., 1000 for fixed steps/epoch\n",
    "\n",
    "# For no worker restart behavior, keep num_workers > 0 and persistent_workers=True.\n",
    "# Lower this if your machine is memory constrained.\n",
    "num_workers = 30\n",
    "persistent_workers = True\n",
    "\n",
    "checkpoint_dir = os.path.join('checkpoints', 'contrastive_regular')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load metadata + build train/test datasets ----\n",
    "ap = ActivationParser(\n",
    "    inference_json=inference_json,\n",
    "    eval_json=eval_json,\n",
    "    activations_path=activations_path,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "train_dataset = ap.get_dataset(\n",
    "    'train',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "test_dataset = ap.get_dataset(\n",
    "    'test',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "print('train:', len(train_dataset))\n",
    "print('test :', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Regular contrastive encoder ----\n",
    "model = ProgressiveCompressor(\n",
    "    input_dim=input_dim,\n",
    "    final_dim=final_dim,\n",
    "    input_dropout=0.3,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train with the new Trainer API ----\n",
    "config = ContrastiveTrainerConfig(\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    temperature=temperature,\n",
    "    steps_per_epoch_override=steps_per_epoch_override,\n",
    "    device=device,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=persistent_workers,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    save_every=1,\n",
    "    snapshot_every=10,\n",
    "    snapshot_keep_last=5,\n",
    "\n",
    "    # Supervised contrastive: uses `halu` labels; ignore the configured outlier class.\n",
    "    use_labels=True,\n",
    "    ignore_label=outlier_class,\n",
    "\n",
    "    # Keeps DataLoader workers alive for map-style datasets (disabled automatically for IterableDataset).\n",
    "    use_infinite_index_stream=True,\n",
    "    use_infinite_index_stream_eval=True,\n",
    ")\n",
    "print(config)\n",
    "\n",
    "trainer = ContrastiveTrainer(model, config=config)\n",
    "trainer.fit(train_dataset=train_dataset, val_dataset=test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ea4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OOD evaluation with the new evaluator abstraction ----\n",
    "# Build baseline/eval datasets for embedding-based OOD metrics.\n",
    "train_dataset_for_inference = ap.get_dataset(\n",
    "    'train',\n",
    "    relevant_layers=target_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "eval_dataset = ap.get_dataset(\n",
    "    'test',\n",
    "    relevant_layers=target_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "# DataLoaders are used by the evaluator primarily for the `.dataset` attribute.\n",
    "train_loader_for_baseline = DataLoader(train_dataset_for_inference, batch_size=64, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model_for_eval = trainer.model  # already on the right device\n",
    "\n",
    "# Multi-metric OOD evaluation using shared embeddings (computed once).\n",
    "# - MDS defaults to ID-only baseline (non-outlier class center).\n",
    "# - KNN uses all baseline records, defaults to k=50, and can calibrate k.\n",
    "ood_eval = MultiMetricHallucinationEvaluator(\n",
    "    activation_parser_df=ap.df,\n",
    "    train_data_loader=train_loader_for_baseline,\n",
    "    layers=None,\n",
    "    batch_size=256,\n",
    "    sub_batch_size=64,\n",
    "    device=str(trainer.device),\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=False,\n",
    "    outlier_class=outlier_class,\n",
    "    metrics=[\n",
    "        'cosine',\n",
    "        'mds',\n",
    "        {\n",
    "            'metric': 'knn',\n",
    "            'kwargs': {\n",
    "                'k': 50,\n",
    "                'metric': 'euclidean',\n",
    "                'calibrate_k': True,\n",
    "                'k_candidates': [50, 100, 200, 500, 1000],\n",
    "                'max_train_size': 200000,\n",
    "                'sample_seed': 0,\n",
    "            },\n",
    "            'train_selection': 'all',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "ood_stats = ood_eval.compute(eval_loader, model_for_eval)\n",
    "print('OOD metrics:', ood_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfa3d8",
   "metadata": {},
   "source": [
    "## Evaluate across epoch snapshots\n",
    "\n",
    "Load each saved snapshot checkpoint, run OOD evaluation, and compare metrics across training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accad05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def _slugify_component(name: str) -> str:\n",
    "    cleaned = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", str(name).strip().lower())\n",
    "    return cleaned.strip(\"_\") or \"unknown\"\n",
    "\n",
    "# ---- Discover available snapshots ----\n",
    "trainer_name = type(trainer).__name__ if 'trainer' in dir() else 'ContrastiveTrainer'\n",
    "model_obj = trainer.model if 'trainer' in dir() else (model if 'model' in dir() else None)\n",
    "model_name = model_obj.__class__.__name__ if model_obj is not None else 'ProgressiveCompressor'\n",
    "configured_subdir = getattr(config, 'snapshot_subdir', None) if 'config' in dir() else None\n",
    "\n",
    "if configured_subdir is None:\n",
    "    snapshot_subdir = f\"{_slugify_component(trainer_name)}__{_slugify_component(model_name)}\"\n",
    "elif str(configured_subdir).strip() == \"\":\n",
    "    snapshot_subdir = \"\"\n",
    "else:\n",
    "    snapshot_subdir = str(configured_subdir).strip()\n",
    "\n",
    "snapshot_dir = checkpoint_dir if snapshot_subdir == \"\" else os.path.join(checkpoint_dir, snapshot_subdir)\n",
    "snapshot_pattern = os.path.join(snapshot_dir, \"contrastive_snapshot_epoch_*.pt\")\n",
    "snapshot_files = sorted(glob.glob(snapshot_pattern))\n",
    "\n",
    "# Backward compatibility: include legacy root-level snapshot layout\n",
    "legacy_snapshot_pattern = os.path.join(checkpoint_dir, \"contrastive_snapshot_epoch_*.pt\")\n",
    "legacy_snapshot_files = sorted(glob.glob(legacy_snapshot_pattern))\n",
    "snapshot_files = sorted(set(snapshot_files + legacy_snapshot_files))\n",
    "\n",
    "# Also include contrastive_last.pt from the checkpoint root\n",
    "last_ckpt = os.path.join(checkpoint_dir, \"contrastive_last.pt\")\n",
    "if os.path.exists(last_ckpt):\n",
    "    snapshot_files.append(last_ckpt)\n",
    "snapshot_files = sorted(set(snapshot_files))\n",
    "\n",
    "# Parse epoch numbers from filenames for display\n",
    "def _parse_epoch(path: str) -> str:\n",
    "    \"\"\"Extract a human-readable epoch label from a checkpoint filename.\"\"\"\n",
    "    basename = os.path.basename(path)\n",
    "    m = re.search(r\"epoch_(\\d+)\", basename)\n",
    "    if m:\n",
    "        return f\"epoch_{int(m.group(1))}\"\n",
    "    if \"last\" in basename:\n",
    "        return \"last\"\n",
    "    return basename\n",
    "\n",
    "snapshot_info = [(p, _parse_epoch(p)) for p in snapshot_files]\n",
    "print(f\"Resolved snapshot directory: {snapshot_dir}\")\n",
    "print(f\"Found {len(snapshot_info)} checkpoint(s) to evaluate:\")\n",
    "for path, label in snapshot_info:\n",
    "    print(f\"  {label:>12s}  ->  {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluate each snapshot ----\n",
    "# Re-use the same eval datasets and evaluator config from above,\n",
    "# but swap out the model weights for each snapshot.\n",
    "\n",
    "# Build eval datasets (same target_layers as the single-epoch evaluation)\n",
    "if 'train_dataset_for_inference' not in dir():\n",
    "    train_dataset_for_inference = ap.get_dataset(\n",
    "        'train', relevant_layers=target_layers, fixed_layer=fixed_layer, backend=backend,\n",
    "    )\n",
    "if 'eval_dataset' not in dir():\n",
    "    eval_dataset = ap.get_dataset(\n",
    "        'test', relevant_layers=target_layers, fixed_layer=fixed_layer, backend=backend,\n",
    "    )\n",
    "\n",
    "train_loader_for_baseline = DataLoader(train_dataset_for_inference, batch_size=64, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for ckpt_path, epoch_label in tqdm(snapshot_info, desc=\"Evaluating snapshots\"):\n",
    "    # Load model weights from checkpoint\n",
    "    ckpt = torch.load(ckpt_path, map_location=device if device != 'auto' else 'cuda')\n",
    "    \n",
    "    # Fresh model instance so we don't leak state between snapshots\n",
    "    snapshot_model = ProgressiveCompressor(input_dim=input_dim, final_dim=final_dim, input_dropout=0.3)\n",
    "    snapshot_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    eval_device = str(trainer.device) if 'trainer' in dir() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    snapshot_model = snapshot_model.to(eval_device)\n",
    "    snapshot_model.eval()\n",
    "\n",
    "    # Run multi-metric OOD evaluation\n",
    "    ood_eval = MultiMetricHallucinationEvaluator(\n",
    "        activation_parser_df=ap.df,\n",
    "        train_data_loader=train_loader_for_baseline,\n",
    "        layers=None,\n",
    "        batch_size=256,\n",
    "        sub_batch_size=64,\n",
    "        device=eval_device,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=False,\n",
    "        outlier_class=outlier_class,\n",
    "        metrics=[\n",
    "            'cosine',\n",
    "            'mds',\n",
    "            {\n",
    "                'metric': 'knn',\n",
    "                'kwargs': {\n",
    "                    'k': 50,\n",
    "                    'metric': 'euclidean',\n",
    "                    'calibrate_k': True,\n",
    "                    'k_candidates': [50, 100, 200, 500, 1000],\n",
    "                    'max_train_size': 200000,\n",
    "                    'sample_seed': 0,\n",
    "                },\n",
    "                'train_selection': 'all',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    stats = ood_eval.compute(eval_loader, snapshot_model)\n",
    "\n",
    "    # Extract the epoch number for sorting (numeric); fall back to a large value for \"last\"\n",
    "    epoch_num = int(re.search(r\"\\d+\", epoch_label).group()) if re.search(r\"\\d+\", epoch_label) else 9999\n",
    "    result_row = {\"checkpoint\": epoch_label, \"epoch\": epoch_num, \"path\": ckpt_path}\n",
    "    result_row.update(stats)\n",
    "    all_results.append(result_row)\n",
    "\n",
    "    logger.info(f\"[{epoch_label}] {stats}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values(\"epoch\").reset_index(drop=True)\n",
    "print(\"\\n=== Results across snapshots ===\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize metrics across epochs ----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify numeric metric columns (exclude metadata columns)\n",
    "meta_cols = {'checkpoint', 'epoch', 'path'}\n",
    "metric_cols = [c for c in results_df.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(results_df[c])]\n",
    "\n",
    "if not metric_cols:\n",
    "    print(\"No numeric metric columns found to plot.\")\n",
    "else:\n",
    "    n_metrics = len(metric_cols)\n",
    "    fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 4), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, col in zip(axes, metric_cols):\n",
    "        ax.plot(results_df[\"epoch\"], results_df[col], marker=\"o\", linewidth=1.5)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(col)\n",
    "        ax.set_title(col)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"OOD Metrics vs Training Epoch\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
