{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366debde",
   "metadata": {},
   "source": [
    "# K-view loader profiling (k=2, CPU-only)\n",
    "\n",
    "This notebook compares data loading performance between:\n",
    "- Current K-view loader (`ActivationDataset` via `ActivationParser.get_dataset(..., num_views=2)`)\n",
    "- Legacy two-view loader (`LegacyActivationDataset`)\n",
    "\n",
    "The setup mirrors `b_contrastive_training_with_new_trainer.ipynb` path/layer/loader settings, but does not run model forward/backward and does not use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from statistics import median\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from activation_logging.activation_parser import ActivationParser\n",
    "from activation_logging.legacy_activation_dataset import LegacyActivationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97377bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths (match training notebook defaults) ----\n",
    "inference_json = 'shared/goodwiki_jsonv2/generation.jsonl'\n",
    "eval_json = 'shared/goodwiki.zarr/eval_results.json'\n",
    "activations_path = 'shared/goodwiki.zarr/activations.zarr'\n",
    "\n",
    "# ---- Dataset parameters ----\n",
    "backend = 'zarr'\n",
    "relevant_layers = list(range(14, 30))\n",
    "fixed_layer = None\n",
    "pad_length = 63\n",
    "min_target_layers = 2\n",
    "num_views = 2  # profile exactly k=2\n",
    "\n",
    "# ---- DataLoader parameters (similar conditions to training notebook) ----\n",
    "batch_size = 512\n",
    "num_workers_candidates = [4, 16]\n",
    "persistent_workers = True\n",
    "prefetch_factor = 2\n",
    "\n",
    "# ---- Profiling controls ----\n",
    "seed = 42\n",
    "getitem_samples = 2048\n",
    "dataloader_batches = 40\n",
    "warmup_batches = 5\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "print('cpu count:', os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77abb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build parser and datasets ----\n",
    "ap = ActivationParser(\n",
    "    inference_json=inference_json,\n",
    "    eval_json=eval_json,\n",
    "    activations_path=activations_path,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "current_train = ap.get_dataset(\n",
    "    'train',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    pad_length=pad_length,\n",
    "    min_target_layers=min_target_layers,\n",
    "    num_views=num_views,\n",
    "    backend=backend,\n",
    ")\n",
    "\n",
    "legacy_train = LegacyActivationDataset(\n",
    "    df=ap.df,\n",
    "    activations_path=activations_path,\n",
    "    split='train',\n",
    "    relevant_layers=relevant_layers,\n",
    "    fixed_layer=fixed_layer,\n",
    "    pad_length=pad_length,\n",
    "    min_target_layers=min_target_layers,\n",
    "    logger_type='zarr',\n",
    "    random_seed=seed,\n",
    "    verbose=False,\n",
    "    return_all_activations=False,\n",
    ")\n",
    "\n",
    "print('train size (current):', len(current_train))\n",
    "print('train size (legacy) :', len(legacy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a04eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summary_from_durations(durations, units='sample'):\n",
    "    arr = np.asarray(durations, dtype=np.float64)\n",
    "    total = float(arr.sum())\n",
    "    count = int(arr.size)\n",
    "    return {\n",
    "        'count': count,\n",
    "        f'total_s_per_{units}': total,\n",
    "        f'mean_s_per_{units}': float(arr.mean()) if count else float('nan'),\n",
    "        f'median_s_per_{units}': float(np.median(arr)) if count else float('nan'),\n",
    "        f'p95_s_per_{units}': float(np.percentile(arr, 95)) if count else float('nan'),\n",
    "        f'p99_s_per_{units}': float(np.percentile(arr, 99)) if count else float('nan'),\n",
    "    }\n",
    "\n",
    "\n",
    "def profile_getitem(dataset, n_samples=1024):\n",
    "    n = min(n_samples, len(dataset))\n",
    "    durations = []\n",
    "    for idx in range(n):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = dataset[idx]\n",
    "        durations.append(time.perf_counter() - t0)\n",
    "\n",
    "    summary = _summary_from_durations(durations, units='sample')\n",
    "    summary['samples_per_sec'] = n / summary['total_s_per_sample'] if summary['total_s_per_sample'] > 0 else float('inf')\n",
    "    return summary\n",
    "\n",
    "\n",
    "def _passthrough_collate(batch):\n",
    "    return batch\n",
    "\n",
    "\n",
    "def profile_dataloader(dataset, *, batch_size, num_workers, persistent_workers, prefetch_factor, warmup_batches, timed_batches):\n",
    "    loader_kwargs = dict(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=_passthrough_collate,\n",
    "    )\n",
    "\n",
    "    if num_workers > 0:\n",
    "        loader_kwargs['persistent_workers'] = persistent_workers\n",
    "        loader_kwargs['prefetch_factor'] = prefetch_factor\n",
    "\n",
    "    dl = DataLoader(**loader_kwargs)\n",
    "\n",
    "    it = iter(dl)\n",
    "    for _ in range(warmup_batches):\n",
    "        try:\n",
    "            _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl)\n",
    "            _ = next(it)\n",
    "\n",
    "    batch_durations = []\n",
    "    samples_seen = 0\n",
    "    for _ in range(timed_batches):\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            batch = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl)\n",
    "            batch = next(it)\n",
    "        dt = time.perf_counter() - t0\n",
    "        batch_durations.append(dt)\n",
    "        samples_seen += len(batch)\n",
    "\n",
    "    summary = _summary_from_durations(batch_durations, units='batch')\n",
    "    total_t = summary['total_s_per_batch']\n",
    "    summary['samples_seen'] = samples_seen\n",
    "    summary['samples_per_sec'] = samples_seen / total_t if total_t > 0 else float('inf')\n",
    "    summary['batches_per_sec'] = len(batch_durations) / total_t if total_t > 0 else float('inf')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Pure __getitem__ timing (single-process baseline) ----\n",
    "getitem_results = []\n",
    "for name, ds in [('current_k2', current_train), ('legacy_2view', legacy_train)]:\n",
    "    summary = profile_getitem(ds, n_samples=getitem_samples)\n",
    "    summary['loader'] = name\n",
    "    getitem_results.append(summary)\n",
    "\n",
    "getitem_df = pd.DataFrame(getitem_results).set_index('loader').sort_index()\n",
    "getitem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2) DataLoader timing sweep (multi-worker) ----\n",
    "rows = []\n",
    "for nw in num_workers_candidates:\n",
    "    for name, ds in [('current_k2', current_train), ('legacy_2view', legacy_train)]:\n",
    "        summary = profile_dataloader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=nw,\n",
    "            persistent_workers=(persistent_workers and nw > 0),\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            warmup_batches=warmup_batches,\n",
    "            timed_batches=dataloader_batches,\n",
    "        )\n",
    "        summary['loader'] = name\n",
    "        summary['num_workers'] = nw\n",
    "        rows.append(summary)\n",
    "        print(f\"done: loader={name} num_workers={nw} samples/s={summary['samples_per_sec']:.2f}\")\n",
    "\n",
    "dl_df = pd.DataFrame(rows)\n",
    "dl_df = dl_df.sort_values(['num_workers', 'loader']).reset_index(drop=True)\n",
    "dl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90713b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Relative comparison table ----\n",
    "pivot = dl_df.pivot(index='num_workers', columns='loader', values='samples_per_sec')\n",
    "if {'current_k2', 'legacy_2view'}.issubset(set(pivot.columns)):\n",
    "    pivot['legacy_over_current_speedup'] = pivot['legacy_2view'] / pivot['current_k2']\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7cb9f",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- This profile isolates data loading; model compute is intentionally excluded.\n",
    "- The current K-view loader with `num_views=2` may still load all relevant layers in its Zarr path, which can dominate IO.\n",
    "- If the machine cannot handle high worker counts, reduce `num_workers_candidates`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
